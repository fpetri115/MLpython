{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"imdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "\n",
    "reviews_test = load_files(\"imdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Unsupervised Approaches\n",
    "\n",
    "There are a number of unsupervised approaches in NLP which can give us useful information about the substructure of a corpus, or suggest relevance rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling\n",
    "\n",
    "The idea of *topic modelling* is to assign each document to one or more topic classes. For example, news data can usually be assigned to *politics*, *sport*, *finance*, etc.\n",
    "\n",
    "The *Latent Dirichlet Allocation* (LDA) approach models each document as a weighted *mixture* of topics. Topics will be derived from groups of words that tend to occur together. For example, \"MP\", \"minister\" and \"vote\" will tend to be found together, while \"goal\", \"team\" and \"score\" will tend to be found together. Because this is an unsupervised approach, the topics obtained may not coincide with our understanding of the relevant groups withing the corpus. However, LDA can still be a useful way to reduce the dimensionality of the data for further exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each topic (a row in the components_), sort the features (ascending).\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# get the feature names from the vectorizer:\n",
    "feature_names = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top-weighted features for the ten topics identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in range(len(sorting)):\n",
    "    features = feature_names[sorting[topic_id]]\n",
    "    print( \"topic\", topic_id, features[0:5] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a larger number of topics to get finer resolution on the substructure of the corpus.\n",
    "\n",
    "If few training data are available, transforming into the lower-dimensional LDA feature space may be a helpful preprocessing step for supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by weight of \"horror\" topic 7\n",
    "horror = np.argsort(document_topics[:, 7])[::-1]\n",
    "# print the five documents where the topic is most important\n",
    "for i in horror[:5]:\n",
    "    # show first two sentences\n",
    "    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Documents\n",
    "\n",
    "We can imagine using a distance metric within the *bag of words*, *n-grams* or LDA feature space to find documents that are \"similar\" to a given input document.\n",
    "\n",
    "However, these features are not good at capturing the *semantic* similarity between documents. Consider\n",
    "\n",
    "* The cat sat on the mat\n",
    "* The feline rested on the rug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2vec` is a very popular approach to this problem that is based on neural networks. Using a large unannotated plain text corpus, `word2vec` constructs a *continuous* feature space with a predefined dimensionality, within which individual words are located. Words with similar semantic meaning are located close together in this feature space. More remarkably, certain linear relationships can be identified:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `gensim` package to explore some of the possibilities of this approach.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html\n",
    "\n",
    "Firstly, we will download a pre-trained model with 100 dimensions, trained on 400,000 Wikipedia articles from 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives the vector representation of any word from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['potato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['potato'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can compute similarities to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"potato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By embedding documents into this space, document similarity can be computed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
